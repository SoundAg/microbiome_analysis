{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SoundAg/microbiome_analysis/blob/main/2022_11_17_151401_Data_Processing_UNITE_update.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Google co-lab notebook for ITS rRNA amplicon sequencing  "
      ],
      "metadata": {
        "id": "o6peuUAsLyoP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Get data for tutorial**\n",
        "\n",
        "To access the data needed for this tutorial we need to download [UNITE](https://unite.ut.ee/repository.php) database files (sh_taxonomy_qiime_ver8_dynamic_s_all_10.05.2021.txt, sh_refs_qiime_ver8_dynamic_s_all_10.05.2021.fasta).\n",
        "\n",
        "The demultiplexed fasta reads which allow you to skip to **Mapping to reference database** can be found on ncbi PRJNA785658 (SRR22220775-SRR22220777. \n",
        "\n",
        "Versions of barcode demultiplex scripts used in processing raw nanopore reads can be found on [github](https://github.com/krasileva-group/Duckweed-Microbiome.git).  \n",
        "\n",
        "Once you have downloaded the data put it in your google drive or load into the colab session. "
      ],
      "metadata": {
        "id": "B0KSmDqXgwLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Click on left panel, navigate to folder icon on the far left. Then at the top of the file bar click on the folder with the google drive symbol and agree to mount google drive. "
      ],
      "metadata": {
        "id": "6ICOX6u_GXjL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steps for analyzing raw reads (skip if downloaded fasta from SRA)"
      ],
      "metadata": {
        "id": "APirbbAWhhGb"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9a7MtewFIAH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "534b3d82-d08d-4536-d53b-aa2d309bf03d"
      },
      "source": [
        "# The Google Colab Environment does not have conda set, this would\n",
        "# ordinarily be the easies option to install these tools.\n",
        "\n",
        "!pip install git+https://github.com/rrwick/Porechop.git  # just so pomoxis will install cleanly\n",
        "!pip install medaka pomoxis aplanat intervaltree==3.0.2\n",
        "# install samtools from source\n",
        "!wget https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2\n",
        "!tar -xjf samtools-1.10.tar.bz2\n",
        "!cd samtools-1.10 && ./configure --prefix=/usr/local/ && make && make install\n",
        "!wget https://github.com/lh3/minimap2/releases/download/v2.17/minimap2-2.17_x64-linux.tar.bz2\n",
        "!tar -xjf minimap2-2.17_x64-linux.tar.bz2\n",
        "!cp /content/minimap2-2.17_x64-linux/minimap2 /usr/local/bin/\n",
        "!pip install requests "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/rrwick/Porechop.git\n",
            "  Cloning https://github.com/rrwick/Porechop.git to /tmp/pip-req-build-jz920lrz\n",
            "  Running command git clone -q https://github.com/rrwick/Porechop.git /tmp/pip-req-build-jz920lrz\n",
            "Building wheels for collected packages: porechop\n",
            "  Building wheel for porechop (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for porechop: filename=porechop-0.2.4-py3-none-any.whl size=81941 sha256=a166b9ea69328f37174360fbde6f76fdbaab5955ed7111e17ea3d22150ab691b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7gnbobnd/wheels/a0/0e/3f/a4f2e5fd0e20727599d3ccaa1fa027d5acdb4ce5887794b5bd\n",
            "Successfully built porechop\n",
            "Installing collected packages: porechop\n",
            "Successfully installed porechop-0.2.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting medaka\n",
            "  Downloading medaka-1.7.2-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (41.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 41.0 MB 1.2 MB/s \n",
            "\u001b[?25hCollecting pomoxis\n",
            "  Downloading pomoxis-0.3.11.tar.gz (95 kB)\n",
            "\u001b[K     |████████████████████████████████| 95 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting aplanat\n",
            "  Downloading aplanat-0.6.15.tar.gz (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 52.6 MB/s \n",
            "\u001b[?25hCollecting intervaltree==3.0.2\n",
            "  Downloading intervaltree-3.0.2.tar.gz (30 kB)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from intervaltree==3.0.2) (2.4.0)\n",
            "Collecting ont-fast5-api\n",
            "  Downloading ont_fast5_api-4.1.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from medaka) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from medaka) (2.23.0)\n",
            "Collecting tensorflow~=2.7.0\n",
            "  Downloading tensorflow-2.7.4-cp38-cp38-manylinux2010_x86_64.whl (496.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 496.0 MB 9.9 kB/s \n",
            "\u001b[?25hCollecting pysam>=0.16.0.1\n",
            "  Downloading pysam-0.20.0-cp38-cp38-manylinux_2_24_x86_64.whl (16.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.1 MB 35.8 MB/s \n",
            "\u001b[?25hCollecting cffi==1.15.0\n",
            "  Downloading cffi-1.15.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (446 kB)\n",
            "\u001b[K     |████████████████████████████████| 446 kB 69.0 MB/s \n",
            "\u001b[?25hCollecting mappy\n",
            "  Downloading mappy-2.24.tar.gz (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 71.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio in /usr/local/lib/python3.8/dist-packages (from medaka) (1.51.1)\n",
            "Collecting edlib\n",
            "  Downloading edlib-1.3.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (359 kB)\n",
            "\u001b[K     |████████████████████████████████| 359 kB 57.4 MB/s \n",
            "\u001b[?25hCollecting parasail\n",
            "  Downloading parasail-1.3.3-py2.py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 15.6 MB 15.7 MB/s \n",
            "\u001b[?25hCollecting pyspoa>=0.0.3\n",
            "  Downloading pyspoa-0.0.8-cp38-cp38-manylinux2010_x86_64.whl (866 kB)\n",
            "\u001b[K     |████████████████████████████████| 866 kB 43.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (from medaka) (3.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.8/dist-packages (from cffi==1.15.0->medaka) (2.21)\n",
            "Collecting pybind11>=2.4\n",
            "  Downloading pybind11-2.10.1-py3-none-any.whl (216 kB)\n",
            "\u001b[K     |████████████████████████████████| 216 kB 54.8 MB/s \n",
            "\u001b[?25hCollecting cmake==3.18.4\n",
            "  Downloading cmake-3.18.4-py3-none-manylinux1_x86_64.whl (17.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 17.7 MB 472 kB/s \n",
            "\u001b[?25hRequirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (2.1.1)\n",
            "Collecting tensorflow-estimator<2.8,~=2.7.0rc0\n",
            "  Downloading tensorflow_estimator-2.7.0-py2.py3-none-any.whl (463 kB)\n",
            "\u001b[K     |████████████████████████████████| 463 kB 58.5 MB/s \n",
            "\u001b[?25hCollecting keras<2.8,>=2.7.0rc0\n",
            "  Downloading keras-2.7.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 40.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (14.0.6)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (1.3.0)\n",
            "Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (0.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (0.38.4)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (2.9.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (0.2.0)\n",
            "Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (1.12)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (4.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (0.28.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (3.19.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (1.1.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow~=2.7.0->medaka) (1.14.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (2.15.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (0.6.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (57.4.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (3.4.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard~=2.6->tensorflow~=2.7.0->medaka) (1.0.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (4.13.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->medaka) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->medaka) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->medaka) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->medaka) (2022.9.24)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow~=2.7.0->medaka) (3.2.2)\n",
            "Collecting biopython<1.77,>=1.63\n",
            "  Downloading biopython-1.76-cp38-cp38-manylinux1_x86_64.whl (2.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 45.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from pomoxis) (3.2.2)\n",
            "Collecting ncls\n",
            "  Downloading ncls-0.0.65.tar.gz (470 kB)\n",
            "\u001b[K     |████████████████████████████████| 470 kB 65.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from pomoxis) (1.3.5)\n",
            "Requirement already satisfied: Porechop in /usr/local/lib/python3.8/dist-packages (from pomoxis) (0.2.4)\n",
            "Requirement already satisfied: bokeh<3.0.0 in /usr/local/lib/python3.8/dist-packages (from aplanat) (2.3.3)\n",
            "Collecting icon_font_to_png\n",
            "  Downloading icon_font_to_png-0.4.1-py2.py3-none-any.whl (161 kB)\n",
            "\u001b[K     |████████████████████████████████| 161 kB 56.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.8/dist-packages (from aplanat) (7.1.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (from aplanat) (1.7.3)\n",
            "Collecting si-prefix\n",
            "  Downloading si-prefix-1.2.2.tar.gz (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 3.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.8/dist-packages (from aplanat) (1.0.2)\n",
            "Collecting sigfig\n",
            "  Downloading sigfig-1.3.2-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.8/dist-packages (from bokeh<3.0.0->aplanat) (6.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.8/dist-packages (from bokeh<3.0.0->aplanat) (21.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.8/dist-packages (from bokeh<3.0.0->aplanat) (6.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from bokeh<3.0.0->aplanat) (2.8.2)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.8/dist-packages (from bokeh<3.0.0->aplanat) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from Jinja2>=2.9->bokeh<3.0.0->aplanat) (2.0.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=16.8->bokeh<3.0.0->aplanat) (3.0.9)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->pomoxis) (2022.6)\n",
            "Collecting tinycss>=0.4\n",
            "  Downloading tinycss-0.4.tar.gz (87 kB)\n",
            "\u001b[K     |████████████████████████████████| 87 kB 7.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pomoxis) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->pomoxis) (0.11.0)\n",
            "Collecting progressbar33>=2.3.1\n",
            "  Downloading progressbar33-2.4.tar.gz (10 kB)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->aplanat) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn->aplanat) (3.1.0)\n",
            "Building wheels for collected packages: intervaltree, pomoxis, aplanat, tinycss, mappy, ncls, progressbar33, si-prefix\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.0.2-py3-none-any.whl size=25804 sha256=9fe16c3bd05f2367a3d84c0a784ec4f89988f30f55bc06fe7452b4e2a0586a03\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/31/73/2a37de8f93a2c05a2c19734f7039756d3eb1f9de85971b86df\n",
            "  Building wheel for pomoxis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pomoxis: filename=pomoxis-0.3.11-py3-none-any.whl size=61630 sha256=71a1172b3bd1d3e382de4843b8bd44cc26b1008c8d6f8c4d74a85f559311c4da\n",
            "  Stored in directory: /root/.cache/pip/wheels/43/e8/7d/967d76891b967f4f3142dd6c3353aed6de17748d51b08a027e\n",
            "  Building wheel for aplanat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aplanat: filename=aplanat-0.6.15-py3-none-any.whl size=307280 sha256=87a0aa40ef5fb66c1ab7d0f4f1d06daa94039b6d3d32c98f47719fb5c8153adb\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/d7/df/148cde102d92cd994e9e3eb374a9cd9b380bd280609c34a7ac\n",
            "  Building wheel for tinycss (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinycss: filename=tinycss-0.4-py3-none-any.whl size=43955 sha256=8b8d0e0c862e09450ab932c79b16f11415f5cc6f33e77b4d203558b4c532ee7a\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/9e/46/42a1ce4dca43c6b8e0c7ac03eb210872a90fce30f1bd82fa3b\n",
            "  Building wheel for mappy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mappy: filename=mappy-2.24-cp38-cp38-linux_x86_64.whl size=532828 sha256=ed76371dea38a2cbe1c378e15430cfc5c57f2436fc5963cfd32ce25b27bf9662\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/ca/6a/7be4760503025a76f7e52b3487f622e2698ae87598f8d8e0c5\n",
            "  Building wheel for ncls (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ncls: filename=ncls-0.0.65-cp38-cp38-linux_x86_64.whl size=1909446 sha256=83433b790831e005d1ddc933ddde8c3fa349cc5c27eea071c7429931f96825ce\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/9f/02/980a0230888cf6dfa6a2e1fe129ac6ab6efe0ff5e7fe4de517\n",
            "  Building wheel for progressbar33 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar33: filename=progressbar33-2.4-py3-none-any.whl size=12157 sha256=355085d75827a3eee0b65e4f8b5d3601babd95570c32e0cdf7b610920cb974a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/a7/e5/80302edade245519d68bbdf44f7879049f9b67e7bae7ed66e4\n",
            "  Building wheel for si-prefix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for si-prefix: filename=si_prefix-1.2.2-py3-none-any.whl size=5875 sha256=2fafbce95596619b555d9a8ed518e884d772549ca1678f09c5157de7ae97e2b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8f/ab/9c/9edb6046c60626177ffae3e140573a844a799e471878429055\n",
            "Successfully built intervaltree pomoxis aplanat tinycss mappy ncls progressbar33 si-prefix\n",
            "Installing collected packages: tinycss, tensorflow-estimator, pybind11, progressbar33, keras, cmake, tensorflow, sigfig, si-prefix, pyspoa, pysam, parasail, ont-fast5-api, ncls, mappy, intervaltree, icon-font-to-png, edlib, cffi, biopython, pomoxis, medaka, aplanat\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.9.0\n",
            "    Uninstalling tensorflow-estimator-2.9.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.9.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.9.0\n",
            "    Uninstalling keras-2.9.0:\n",
            "      Successfully uninstalled keras-2.9.0\n",
            "  Attempting uninstall: cmake\n",
            "    Found existing installation: cmake 3.22.6\n",
            "    Uninstalling cmake-3.22.6:\n",
            "      Successfully uninstalled cmake-3.22.6\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.9.2\n",
            "    Uninstalling tensorflow-2.9.2:\n",
            "      Successfully uninstalled tensorflow-2.9.2\n",
            "  Attempting uninstall: intervaltree\n",
            "    Found existing installation: intervaltree 2.1.0\n",
            "    Uninstalling intervaltree-2.1.0:\n",
            "      Successfully uninstalled intervaltree-2.1.0\n",
            "  Attempting uninstall: cffi\n",
            "    Found existing installation: cffi 1.15.1\n",
            "    Uninstalling cffi-1.15.1:\n",
            "      Successfully uninstalled cffi-1.15.1\n",
            "Successfully installed aplanat-0.6.15 biopython-1.76 cffi-1.15.0 cmake-3.18.4 edlib-1.3.9 icon-font-to-png-0.4.1 intervaltree-3.0.2 keras-2.7.0 mappy-2.24 medaka-1.7.2 ncls-0.0.65 ont-fast5-api-4.1.0 parasail-1.3.3 pomoxis-0.3.11 progressbar33-2.4 pybind11-2.10.1 pysam-0.20.0 pyspoa-0.0.8 si-prefix-1.2.2 sigfig-1.3.2 tensorflow-2.7.4 tensorflow-estimator-2.7.0 tinycss-0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cffi"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-12-09 22:34:28--  https://github.com/samtools/samtools/releases/download/1.10/samtools-1.10.tar.bz2\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/3666841/207d0e80-1848-11ea-9eb4-eeeea8d26cef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221209T223428Z&X-Amz-Expires=300&X-Amz-Signature=d409876adbed6308b0d29ffa563f461a4e37e984498fc2fd3662caaf4994dbec&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=3666841&response-content-disposition=attachment%3B%20filename%3Dsamtools-1.10.tar.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-12-09 22:34:28--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/3666841/207d0e80-1848-11ea-9eb4-eeeea8d26cef?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221209T223428Z&X-Amz-Expires=300&X-Amz-Signature=d409876adbed6308b0d29ffa563f461a4e37e984498fc2fd3662caaf4994dbec&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=3666841&response-content-disposition=attachment%3B%20filename%3Dsamtools-1.10.tar.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4721173 (4.5M) [application/octet-stream]\n",
            "Saving to: ‘samtools-1.10.tar.bz2’\n",
            "\n",
            "samtools-1.10.tar.b 100%[===================>]   4.50M  5.41MB/s    in 0.8s    \n",
            "\n",
            "2022-12-09 22:34:29 (5.41 MB/s) - ‘samtools-1.10.tar.bz2’ saved [4721173/4721173]\n",
            "\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for C compiler warning flags... -Wall\n",
            "checking for special C compiler options needed for large files... no\n",
            "checking for _FILE_OFFSET_BITS value needed for large files... no\n",
            "checking location of HTSlib source tree... htslib-1.10\n",
            "checking for NcursesW wide-character library... no\n",
            "checking for Ncurses library... yes\n",
            "checking for working ncurses/curses.h... no\n",
            "checking for working ncurses.h... yes\n",
            "checking for zlib.h... yes\n",
            "checking for inflate in -lz... yes\n",
            "checking for library containing regcomp... none required\n",
            "configure: creating ./config.status\n",
            "config.status: creating config.mk\n",
            "config.status: creating config.h\n",
            "=== configuring in htslib-1.10 (/content/samtools-1.10/htslib-1.10)\n",
            "configure: running /bin/bash ./configure --disable-option-checking '--prefix=/usr/local'  --cache-file=/dev/null --srcdir=.\n",
            "checking for gcc... gcc\n",
            "checking whether the C compiler works... yes\n",
            "checking for C compiler default output file name... a.out\n",
            "checking for suffix of executables... \n",
            "checking whether we are cross compiling... no\n",
            "checking for suffix of object files... o\n",
            "checking whether we are using the GNU C compiler... yes\n",
            "checking whether gcc accepts -g... yes\n",
            "checking for gcc option to accept ISO C89... none needed\n",
            "checking for ranlib... ranlib\n",
            "checking for grep that handles long lines and -e... /bin/grep\n",
            "checking for C compiler warning flags... -Wall\n",
            "checking for pkg-config... /usr/bin/pkg-config\n",
            "checking pkg-config is at least version 0.9.0... yes\n",
            "checking for special C compiler options needed for large files... no\n",
            "checking for _FILE_OFFSET_BITS value needed for large files... no\n",
            "checking shared library type for unknown-Linux... plain .so\n",
            "checking whether the compiler accepts -fvisibility=hidden... yes\n",
            "checking how to run the C preprocessor... gcc -E\n",
            "checking for egrep... /bin/grep -E\n",
            "checking for ANSI C header files... yes\n",
            "checking for sys/types.h... yes\n",
            "checking for sys/stat.h... yes\n",
            "checking for stdlib.h... yes\n",
            "checking for string.h... yes\n",
            "checking for memory.h... yes\n",
            "checking for strings.h... yes\n",
            "checking for inttypes.h... yes\n",
            "checking for stdint.h... yes\n",
            "checking for unistd.h... yes\n",
            "checking for stdlib.h... (cached) yes\n",
            "checking for unistd.h... (cached) yes\n",
            "checking for sys/param.h... yes\n",
            "checking for getpagesize... yes\n",
            "checking for working mmap... yes\n",
            "checking for gmtime_r... yes\n",
            "checking for fsync... yes\n",
            "checking for drand48... yes\n",
            "checking whether fdatasync is declared... yes\n",
            "checking for fdatasync... yes\n",
            "checking for library containing log... -lm\n",
            "checking for zlib.h... yes\n",
            "checking for inflate in -lz... yes\n",
            "checking for library containing recv... none required\n",
            "checking for bzlib.h... yes\n",
            "checking for BZ2_bzBuffToBuffCompress in -lbz2... yes\n",
            "checking for lzma.h... yes\n",
            "checking for lzma_easy_buffer_encode in -llzma... yes\n",
            "checking for libdeflate.h... no\n",
            "checking for libdeflate_deflate_compress in -ldeflate... no\n",
            "checking for curl_easy_pause in -lcurl... yes\n",
            "checking for CCHmac... no\n",
            "checking for library containing HMAC... -lcrypto\n",
            "checking whether PTHREAD_MUTEX_RECURSIVE is declared... yes\n",
            "configure: creating ./config.status\n",
            "config.status: creating config.mk\n",
            "config.status: creating htslib.pc.tmp\n",
            "config.status: creating config.h\n",
            "config.mk:45: htslib-1.10/htslib_static.mk: No such file or directory\n",
            "cd htslib-1.10 && make htslib_static.mk\n",
            "make[1]: Entering directory '/content/samtools-1.10/htslib-1.10'\n",
            "sed -n '/^static_libs=/s/[^=]*=/HTSLIB_static_LIBS = /p;/^static_ldflags=/s/[^=]*=/HTSLIB_static_LDFLAGS = /p' htslib.pc.tmp > htslib_static.mk\n",
            "make[1]: Leaving directory '/content/samtools-1.10/htslib-1.10'\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_index.o bam_index.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_plcmd.o bam_plcmd.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sam_view.o sam_view.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_fastq.o bam_fastq.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_cat.o bam_cat.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_md.o bam_md.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_reheader.o bam_reheader.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_sort.o bam_sort.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bedidx.o bedidx.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_rmdup.o bam_rmdup.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_rmdupse.o bam_rmdupse.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_mate.o bam_mate.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_stat.o bam_stat.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_color.o bam_color.c\n",
            "echo '#define SAMTOOLS_VERSION \"1.10\"' > version.h\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bamtk.o bamtk.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam2bcf.o bam2bcf.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam2bcf_indel.o bam2bcf_indel.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sample.o sample.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o cut_target.o cut_target.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o phase.o phase.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam2depth.o bam2depth.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o coverage.o coverage.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o padding.o padding.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bedcov.o bedcov.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bamshuf.o bamshuf.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o faidx.o faidx.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o dict.o dict.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o stats.o stats.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o stats_isize.o stats_isize.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_flags.o bam_flags.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_split.o bam_split.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_tview.o bam_tview.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_tview_curses.o bam_tview_curses.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_tview_html.o bam_tview_html.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_lpileup.o bam_lpileup.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_quickcheck.o bam_quickcheck.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_addrprg.o bam_addrprg.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_markdup.o bam_markdup.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o tmp_file.o tmp_file.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o lz4/lz4.o lz4/lz4.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_aux.o bam_aux.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam.o bam.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sam.o sam.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o bam_plbuf.o bam_plbuf.c\n",
            "ar -csru libbam.a bam_aux.o bam.o sam.o bam_plbuf.o\n",
            "ar: `u' modifier ignored since `D' is the default (see `U')\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sam_opts.o sam_opts.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o sam_utils.o sam_utils.c\n",
            "ar -rcs libst.a sam_opts.o sam_utils.o\n",
            "cd htslib-1.10 && make hts-object-files\n",
            "make[1]: Entering directory '/content/samtools-1.10/htslib-1.10'\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o kfunc.o kfunc.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o knetfile.o knetfile.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o kstring.o kstring.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o bcf_sr_sort.o bcf_sr_sort.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o bgzf.o bgzf.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o errmod.o errmod.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o faidx.o faidx.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o header.o header.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile.o hfile.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_net.o hfile_net.c\n",
            "echo '#define HTS_VERSION_TEXT \"1.10\"' > version.h\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hts.o hts.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hts_os.o hts_os.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o md5.o md5.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o multipart.o multipart.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o probaln.o probaln.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o realn.o realn.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o regidx.o regidx.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o region.o region.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o sam.o sam.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o synced_bcf_reader.o synced_bcf_reader.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o vcf_sweep.o vcf_sweep.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o tbx.o tbx.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o textutils.o textutils.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o thread_pool.o thread_pool.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o vcf.o vcf.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o vcfutils.o vcfutils.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_codecs.o cram/cram_codecs.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_decode.o cram/cram_decode.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_encode.o cram/cram_encode.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_external.o cram/cram_external.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_index.o cram/cram_index.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_io.o cram/cram_io.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_samtools.o cram/cram_samtools.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/cram_stats.o cram/cram_stats.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/mFILE.o cram/mFILE.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/open_trace_file.o cram/open_trace_file.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/pooled_alloc.o cram/pooled_alloc.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/rANS_static.o cram/rANS_static.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o cram/string_alloc.o cram/string_alloc.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_libcurl.o hfile_libcurl.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_gcs.o hfile_gcs.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_s3.o hfile_s3.c\n",
            "gcc -Wall -g -O2 -fvisibility=hidden -I.  -c -o hfile_s3_write.o hfile_s3_write.c\n",
            "touch hts-object-files\n",
            "make[1]: Leaving directory '/content/samtools-1.10/htslib-1.10'\n",
            "cd htslib-1.10 && make lib-static\n",
            "make[1]: Entering directory '/content/samtools-1.10/htslib-1.10'\n",
            "ar -rc libhts.a kfunc.o knetfile.o kstring.o bcf_sr_sort.o bgzf.o errmod.o faidx.o header.o hfile.o hfile_net.o hts.o hts_os.o md5.o multipart.o probaln.o realn.o regidx.o region.o sam.o synced_bcf_reader.o vcf_sweep.o tbx.o textutils.o thread_pool.o vcf.o vcfutils.o cram/cram_codecs.o cram/cram_decode.o cram/cram_encode.o cram/cram_external.o cram/cram_index.o cram/cram_io.o cram/cram_samtools.o cram/cram_stats.o cram/mFILE.o cram/open_trace_file.o cram/pooled_alloc.o cram/rANS_static.o cram/string_alloc.o   hfile_libcurl.o hfile_gcs.o hfile_s3.o hfile_s3_write.o\n",
            "ranlib libhts.a\n",
            "make[1]: Leaving directory '/content/samtools-1.10/htslib-1.10'\n",
            "gcc  -L./lz4  -o samtools bam_index.o bam_plcmd.o sam_view.o bam_fastq.o bam_cat.o bam_md.o bam_reheader.o bam_sort.o bedidx.o bam_rmdup.o bam_rmdupse.o bam_mate.o bam_stat.o bam_color.o bamtk.o bam2bcf.o bam2bcf_indel.o sample.o cut_target.o phase.o bam2depth.o coverage.o padding.o bedcov.o bamshuf.o faidx.o dict.o stats.o stats_isize.o bam_flags.o bam_split.o bam_tview.o bam_tview_curses.o bam_tview_html.o bam_lpileup.o bam_quickcheck.o bam_addrprg.o bam_markdup.o tmp_file.o ./lz4/lz4.o libbam.a libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lncurses -lm -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/ace2sam.o misc/ace2sam.c\n",
            "gcc  -o misc/ace2sam misc/ace2sam.o -lz \n",
            "gcc -Wall -g -O2 -DMAQ_LONGREADS -I. -Ihtslib-1.10 -I./lz4  -c -o misc/maq2sam-long.o misc/maq2sam.c\n",
            "gcc  -o misc/maq2sam-long misc/maq2sam-long.o -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/maq2sam-short.o misc/maq2sam.c\n",
            "gcc  -o misc/maq2sam-short misc/maq2sam-short.o -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/md5fa.o misc/md5fa.c\n",
            "gcc  -L./lz4  -o misc/md5fa misc/md5fa.o htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/md5sum-lite.o misc/md5sum-lite.c\n",
            "gcc  -L./lz4  -o misc/md5sum-lite misc/md5sum-lite.o htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o misc/wgsim.o misc/wgsim.c\n",
            "gcc  -L./lz4  -o misc/wgsim misc/wgsim.o -lm htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz \n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/merge/test_bam_translate.o test/merge/test_bam_translate.c\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/test.o test/test.c\n",
            "gcc  -L./lz4  -o test/merge/test_bam_translate test/merge/test_bam_translate.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/merge/test_rtrans_build.o test/merge/test_rtrans_build.c\n",
            "gcc  -L./lz4  -o test/merge/test_rtrans_build test/merge/test_rtrans_build.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/merge/test_trans_tbl_init.o test/merge/test_trans_tbl_init.c\n",
            "gcc  -L./lz4  -o test/merge/test_trans_tbl_init test/merge/test_trans_tbl_init.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/split/test_count_rg.o test/split/test_count_rg.c\n",
            "gcc  -L./lz4  -o test/split/test_count_rg test/split/test_count_rg.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/split/test_expand_format_string.o test/split/test_expand_format_string.c\n",
            "gcc  -L./lz4  -o test/split/test_expand_format_string test/split/test_expand_format_string.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/split/test_filter_header_rg.o test/split/test_filter_header_rg.c\n",
            "gcc  -L./lz4  -o test/split/test_filter_header_rg test/split/test_filter_header_rg.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/split/test_parse_args.o test/split/test_parse_args.c\n",
            "gcc  -L./lz4  -o test/split/test_parse_args test/split/test_parse_args.o test/test.o libst.a htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "gcc -Wall -g -O2 -I. -Ihtslib-1.10 -I./lz4  -c -o test/vcf-miniview.o test/vcf-miniview.c\n",
            "gcc  -L./lz4  -o test/vcf-miniview test/vcf-miniview.o htslib-1.10/libhts.a -lpthread -lz -lm -lbz2 -llzma -lcurl -lcrypto -lz  -lpthread\n",
            "mkdir -p -m 755 /usr/local/bin /usr/local/bin /usr/local/share/man/man1\n",
            "install -p samtools /usr/local/bin\n",
            "install -p misc/ace2sam misc/maq2sam-long misc/maq2sam-short misc/md5fa misc/md5sum-lite misc/wgsim /usr/local/bin\n",
            "install -p misc/blast2sam.pl misc/bowtie2sam.pl misc/export2sam.pl misc/interpolate_sam.pl misc/novo2sam.pl misc/plot-bamstats misc/psl2sam.pl misc/sam2vcf.pl misc/samtools.pl misc/seq_cache_populate.pl misc/soap2sam.pl misc/wgsim_eval.pl misc/zoom2sam.pl /usr/local/bin\n",
            "install -p -m 644 doc/samtools*.1 misc/wgsim.1 /usr/local/share/man/man1\n",
            "--2022-12-09 22:35:37--  https://github.com/lh3/minimap2/releases/download/v2.17/minimap2-2.17_x64-linux.tar.bz2\n",
            "Resolving github.com (github.com)... 140.82.113.4\n",
            "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/97612481/fafde000-6ec7-11e9-8f99-513f7e53bc2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221209T223537Z&X-Amz-Expires=300&X-Amz-Signature=00935e1b438872e9a8289a7eec68ce891efc190bd6fec644e8208dba259213bb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=97612481&response-content-disposition=attachment%3B%20filename%3Dminimap2-2.17_x64-linux.tar.bz2&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-12-09 22:35:37--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/97612481/fafde000-6ec7-11e9-8f99-513f7e53bc2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20221209%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20221209T223537Z&X-Amz-Expires=300&X-Amz-Signature=00935e1b438872e9a8289a7eec68ce891efc190bd6fec644e8208dba259213bb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=97612481&response-content-disposition=attachment%3B%20filename%3Dminimap2-2.17_x64-linux.tar.bz2&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2100809 (2.0M) [application/octet-stream]\n",
            "Saving to: ‘minimap2-2.17_x64-linux.tar.bz2’\n",
            "\n",
            "minimap2-2.17_x64-l 100%[===================>]   2.00M  3.29MB/s    in 0.6s    \n",
            "\n",
            "2022-12-09 22:35:38 (3.29 MB/s) - ‘minimap2-2.17_x64-linux.tar.bz2’ saved [2100809/2100809]\n",
            "\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests) (2022.9.24)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your data comes already demultiplexed, navigate to the pass_fastq file then create a folder for the barcodes you used. Finally relabel the barcode file names to what they correspond to in terms of experimental samples "
      ],
      "metadata": {
        "id": "Kx-C_5l8Rfgi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###PROBLEM FILE SIZE UPLOAD - COULD CONNECT TO S3 INSTANCE? \n",
        "\n",
        "https://python.plainenglish.io/how-to-load-data-from-aws-s3-into-google-colab-7e76fbf534d2. #OPTION 1 BELOW \n",
        "https://dev.to/zohebabai/boost-your-colab-notebooks-with-gcp-and-aws-instance-within-a-few-minutes-47ma #OR LINK TO EC2 INSTANCE ? MORE COMP POWER?"
      ],
      "metadata": {
        "id": "PIeIFrkGUVub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install boto3 #software development kit to connect AWS and python "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bvmghmS2bsiu",
        "outputId": "9f88055d-60e2-40e0-b82f-9531ad3fb969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.26.27-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting botocore<1.30.0,>=1.29.27\n",
            "  Downloading botocore-1.29.27-py3-none-any.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 27.7 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/dist-packages (from botocore<1.30.0,>=1.29.27->boto3) (2.8.2)\n",
            "Collecting urllib3<1.27,>=1.25.4\n",
            "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 69.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.30.0,>=1.29.27->boto3) (1.15.0)\n",
            "Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "requests 2.23.0 requires urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1, but you have urllib3 1.26.13 which is incompatible.\u001b[0m\n",
            "Successfully installed boto3-1.26.27 botocore-1.29.27 jmespath-1.0.1 s3transfer-0.6.0 urllib3-1.26.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "BUCKET_NAME = 'xxxxxxxxxx' # replace with your bucket name\n",
        "\n",
        "# enter authentication credentials\n",
        "s3 = boto3.resource('s3', aws_access_key_id = 'ENTER YOUR ACCESS KEY', \n",
        "                          aws_secret_access_key= 'ENTER YOUR SECRET KEY')"
      ],
      "metadata": {
        "id": "o2bWveoXcLBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "KEY = 'training.csv' # replace with your object key\n",
        "\n",
        "try:\n",
        "  # we are trying to download training dataset from s3 with name `my-training-data.csv` \n",
        "  # to colab dir with name `training.csv`\n",
        "  s3.Bucket(BUCKET_NAME).download_file(KEY, 'training.csv')\n",
        "  \n",
        "except botocore.exceptions.ClientError as e:\n",
        "  if e.response['Error']['Code'] == \"404\":\n",
        "    print(\"The object does not exist.\")\n",
        "  else:\n",
        "    raise"
      ],
      "metadata": {
        "id": "N0rex-t5cRyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Collapse all fastq to single file and rename then convert fastq to fasta \n"
      ],
      "metadata": {
        "id": "o7TtNCMFuOGu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!for f in */*gz ; do gunzip $f ; done\n",
        "!for f in */ ; do cat $f/* >> $(basename $f).merged  ; done\n",
        "!for f in *merged ; do sed -n '1~4s/^@/>/p;2~4p' $f > $(basename $f .fastq).fasta ; done"
      ],
      "metadata": {
        "id": "olGgFEleNO4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download the UNITE database files from [here](https://unite.ut.ee/repository.php) "
      ],
      "metadata": {
        "id": "j7DWf3PKsDmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for f in *fasta ; do minimap2 -K 5M -ax map-ont -L ../../../../../../Resources/UNITE/UNITE.mmi $f > $(basename $f).sam ; done"
      ],
      "metadata": {
        "id": "-xUyCpxnjz8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#need to add sam sort here as below \n",
        "##!for file in /content/drive/MyDrive/16SColab/sam-indiv/*.sam; do samtools sort -@ 2 -O sam -o /content/drive/MyDrive/16SColab/samsorted-indiv/$(basename $file).sorted $file 2> /content/drive/MyDrive/16SColab/samlog/$(basename $file).indiv.log ; done\n",
        "!for file in *.sam; do echo \"==> ${file} <==\"; grep -v '^@' \"${file}\" > \"${file}.output\"; done"
      ],
      "metadata": {
        "id": "qyBpFxZ0jz8u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Filter reads based on SAM header to keep only those that map and do not have additional SAM [flags](https://broadinstitute.github.io/picard/explain-flags.html).  \n",
        "\n",
        "This was not needed for bacteria, I wonder if its to do with the bacterial database being more complete/ appropriate? \n",
        "\n"
      ],
      "metadata": {
        "id": "N5KQge1Ojz8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir samoutput\n",
        "!cat /content/404.sam.output | awk -F'\\t' '$2 == 0||$2 == 256 {print $0;}' > /content/unite/404.sam.output\n",
        "!cat /content/405.sam.output | awk -F'\\t' '$2 == 0||$2 == 256 {print $0;}' > /content/unite/405.sam.output\n",
        "!cat /content/923.sam.output | awk -F'\\t' '$2 == 0||$2 == 256 {print $0;}' > /content/unite/923.sam.output"
      ],
      "metadata": {
        "id": "T9_t0aihjz8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp *sam.output /content/drive/MyDrive/ITS-Colab/sam/samoutput-30-9-22/  "
      ],
      "metadata": {
        "id": "b-xavVLcuZST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Abundance tables \n",
        "To create abundcance tables we use more of the files from the SILVA DB and some adapted code from the [Puntseq](https://https://github.com/d-j-k/puntseq) project. If interested here is the related paper Urban L (2020), Freshwater monitoring by nanopore sequencing elife [link text](https://elifesciences.org/articles/61504)"
      ],
      "metadata": {
        "id": "uugP6fGxjz8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate table of reads per species\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "sild = pd.read_csv('/Users/erinbaggs/Terminal/Resources/UNITE/sh_taxonomy_qiime_ver8_dynamic_s_all_10.05.2021.txt', sep='\\t', header=None)\n",
        "sild.columns = ['taxid','tree']\n",
        "sild['ranks'] = [x.split(';')[-1:] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "ranks = 'species'\n",
        "# choose dir of sam files\n",
        "dirc = '~/Terminal/Projects/NinaSoil/data/2022_11_17_151401-fungal-fastq_pass/demultiplexed/barcodes_used/cleanoutput' \n",
        "\n",
        "\n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    print(filename)\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('~/Terminal/Projects/NinaSoil/data/2022_11_17_151401-fungal-fastq_pass/demultiplexed/barcodes_used/cleanoutput/%s' %filename, \n",
        "                         sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]  \n",
        "    mini['taxid'] =sild.ranks.loc[mini.id.values].values\n",
        "\n",
        "\n",
        "    if ranks == 'species':\n",
        "        mini['ranks'] = sild.ranks.loc[mini.id.values].values\n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "\n",
        "    mini['ranks']= [(x[0].strip(\"[]\")) for x in mini.ranks] \n",
        "    mini['ranks']= [(x.split(\"s__\")[1]) for x in mini.ranks]     #Current WORKS\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "\n",
        "    if nr==0:\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    nr = nr+1\n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "  \n",
        "minif.to_csv('~/Terminal/Projects/NinaSoil/data/2022_11_17_151401-fungal-fastq_pass/demultiplexed/barcodes_used/minimap_spp_%s.txt' %ranks, sep='\\t')\n"
      ],
      "metadata": {
        "id": "jdsAq4DSjz8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate table of reads per genus\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "sild = pd.read_csv('/content/drive/MyDrive/ITS-Colab/UNITE/sh_taxonomy_qiime_ver8_dynamic_s_all_10.05.2021.txt', sep='\\t', header=None)\n",
        "sild.columns = ['taxid','tree']\n",
        "sild['ranks'] = [x.split(';')[-2:-1] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "ranks = 'genus'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/unite' \n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/unite/%s' %filename, \n",
        "                         sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]  \n",
        "    mini['taxid'] =sild.ranks.loc[mini.id.values].values\n",
        "\n",
        "\n",
        "    if ranks == 'genus':\n",
        "        mini['ranks'] = sild.ranks.loc[mini.id.values].values\n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "\n",
        "    mini['ranks']= [(x[0].strip(\"[]\")) for x in mini.ranks] \n",
        "    mini['ranks']= [(x.split(\"g__\")[1]) for x in mini.ranks]     #Current WORKS\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "\n",
        "    if nr==0:\n",
        "        print('seen')\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "\n",
        "    nr = nr+1\n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "  \n",
        "minif.to_csv('/content/minimap2_unite_%s.txt' %ranks, sep='\\t')\n"
      ],
      "metadata": {
        "id": "bj2ne6Bajz8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate table of reads per family\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "sild = pd.read_csv('/content/drive/MyDrive/ITS-Colab/UNITE/sh_taxonomy_qiime_ver8_dynamic_s_all_10.05.2021.txt', sep='\\t', header=None)\n",
        "sild.columns = ['taxid','tree']\n",
        "sild['ranks'] = [x.split(';')[-4:-3] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "ranks = 'order'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/unite' \n",
        "# create \n",
        "nr = 0\n",
        "print('here')\n",
        "for filename in os.listdir(dirc):\n",
        "    print(filename) \n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/unite/%s' %filename, sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]  \n",
        "    mini['taxid'] =sild.ranks.loc[mini.id.values].values\n",
        "\n",
        "\n",
        "    if ranks == 'order':\n",
        "        mini['ranks'] = sild.ranks.loc[mini.id.values].values\n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "\n",
        "    mini['ranks']= [(x[0].strip(\"[]\")) for x in mini.ranks] \n",
        "    mini['ranks']= [(x.split(\"o__\")[1]) for x in mini.ranks]     #Current WORKS\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "\n",
        "    if nr==0:\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "\n",
        "    nr = nr+1\n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "  \n",
        "minif.to_csv('/content/minimap2_unite_%s.txt' %ranks, sep='\\t')\n"
      ],
      "metadata": {
        "id": "z5Ag9Zbg-JvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate table of reads per genus\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "# load all files from the SILVA database\n",
        "sild = pd.read_csv('/content/drive/MyDrive/ITS-Colab/UNITE/sh_taxonomy_qiime_ver8_dynamic_s_all_10.05.2021.txt', sep='\\t', header=None)\n",
        "sild.columns = ['taxid','tree']\n",
        "sild['ranks'] = [x.split(';')[-6:-5] for x in sild.tree.values]\n",
        "sild['tree'] = [x[0:-1] for x in sild.tree]\n",
        "sild.index = sild.taxid\n",
        "ranks = 'phylum'\n",
        "# choose dir of sam files\n",
        "dirc = '/content/' \n",
        "# create \n",
        "nr = 0\n",
        "for filename in os.listdir(dirc):\n",
        "    \n",
        "    try:\n",
        "        silva_10k = pd.read_csv('/content/%s' %filename, \n",
        "                         sep='\\t', header=None, usecols = [0,2,4,13])\n",
        "    except: \n",
        "        continue\n",
        "    \n",
        "    silva_10k.columns = ['Read_ID', 'id','MS', 'ASs']\n",
        "    silva_10k['ASs'] = silva_10k['ASs'].astype('str')\n",
        "    silva_10k['AS'] = [x.split(':i:')[-1] for x in silva_10k['ASs'].values]\n",
        "    silva_10k.dropna(axis=0, subset=['AS'], inplace=True)\n",
        "    silva_10k['AS'] = silva_10k['AS'].astype('float')\n",
        "    mini = silva_10k[silva_10k['AS'] == silva_10k.groupby('Read_ID')['AS'].transform('max')]\n",
        "    mini = mini[['Read_ID', 'MS', 'AS','id']]              \n",
        "    mini.columns = ['read','score','as','id']\n",
        "    mini = mini[~mini.id.isnull()]  \n",
        "    mini['taxid'] =sild.ranks.loc[mini.id.values].values\n",
        "\n",
        "\n",
        "    if ranks == 'phylum':\n",
        "        mini['ranks'] = sild.ranks.loc[mini.id.values].values\n",
        "        mini.index = mini.read  \n",
        "        for i in mini.index[mini.duplicated(subset='read', keep=False)].unique():\n",
        "            minil = list(mini.loc[i].taxid.values)\n",
        "            if minil.count(minil[0]) != len(minil):\n",
        "                mini.drop(i)\n",
        "        mini.drop_duplicates(subset='read', keep='first', inplace=True)\n",
        "\n",
        "    mini['ranks']= [(x[0].strip(\"[]\")) for x in mini.ranks] \n",
        "    mini['ranks']= [(x.split(\"p__\")[1]) for x in mini.ranks]     #Current WORKS\n",
        "    mini2 = pd.DataFrame(mini.ranks.value_counts())\n",
        "\n",
        "    if nr==0:\n",
        "        minif = mini2.copy(deep=True)\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "    else:\n",
        "        minif = minif.merge(mini2, left_index=True, right_index=True, how='outer')\n",
        "        minif.columns.values[nr] = filename.split('.')[0]\n",
        "\n",
        "    nr = nr+1\n",
        "# describe all missing bacteria as absent\n",
        "minif = minif.fillna(0) \n",
        "  \n",
        "minif.to_csv('/content/minimap2_unite_%s.txt' %ranks, sep='\\t')\n"
      ],
      "metadata": {
        "id": "x1gAIgtD-oGn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}